\chapter{Background and Definitions}
\label{ch:background}
This section introduces the foundational concepts that form the basis of the proposed classification pipeline. It explains the terminology and provides an overview of the core components used throughout this thesis , such as data pipelines , classification and adaptive learning mechanisms. This form the basis of a modern data-driven systems and will be referred to throughout the thesis.

%
% Section: Der erste Abschnitt
%
\section{Data Warehouse (DW)}
\label{sec:background:data_warehouse}
A data warehouse is a centralized repository that integrates data from multiple heterogeneous sources to support decision-making within an organization \cite{vassiliadis:2009}. Its main goal is to provide end-users with consistent, reliable, and unified access to data that would be otherwise fragmented across different systems. To achieve this, incoming data must be transformed into a common schema, cleaned of errors and inconsistencies, and regularly refreshed to reflect updates from production systems \cite{bouaziz:2017}. A typical warehouse is organized into several distinct layers \cite{sajida:2015}:

\begin{description}
  \item[Data source layer:] This includes all types of data providers that feed into the warehouse such as databases, files, as well as semi-autonomous or autonomous systems like APIs or data feeds.
  \item[ETL layer:] In this stage, ETL tools pull data from various sources. The extracted data then undergoes transformations to ensure consistency and quality. Finally, the transformed data is loaded into the targeted system, which is typically the data warehouse storage. The ETL layer is viewed as a data pipeline, as it automates the movement and processing of data across multiple stages before it is made available for analysis.
  \item[Data warehouse storage:] The data warehouse acts as a unified central storage for data produced by the ETL layer. It is primarily a database composed of tables that are combined according to a specific schema. This structured storage allows the data to be later served to end-users in multiple formats, such as reports, dashboards, histograms, or charts, making information accessible for business intelligence.
\end{description}

\section{Traditional ETL Process}
\label{sec:background:etl}
As explained earlier, traditional ETL process is typically represented as a workflow of tasks divided into three major steps \cite{kimball:2004}:

\begin{enumerate}
  \item \textbf{Extraction:} Involves capturing raw data from external sources and moving it into a staging area. At this stage the focus is on pulling the data quickly and reliably, often with minimal restructuring.
  \item \textbf{Transformation:} The extracted data is cleaned, validated, and standardized to ensure accuracy and consistency. This may include removing duplicates and matching the different formats into a unified schema.
  \item \textbf{Loading:} The final step places the transformed data into a data warehouse (DW), where it is structured for analysis. The data is later served to the end-user so they can query it for reports, dashboards, or analytical applications.
\end{enumerate}

These processes are typically organized through ETL jobs. Each job represents a scheduled workflow that automates the pipeline, ensuring the data is processed and delivered to the warehouse in a consistent manner. They usually run according to predefined intervals, which helps maintain data freshness and the reliability of the entire pipeline.





\section{Machine Learning (ML) Pipelines}
\label{sec:background:ml_pipelines}
While ETL pipelines focus on extracting , transforming and loading data into a warehouse for analytics , machine learning (ML) pipelines extend this concept even further by adding extra steps that use the prepared data , to train and deploy predictive models.\cite{lakkarasu:2023} Here are the additional core components that are build upon a traditional ETL pipeline to convert into a full ML pipeline \cite{kramer:2025}:

\begin{description}
  \item[Feature Engineering:] In this step meaningful features are identified from raw data. It ensures that only the most relevant inputs are used for building effective models
  \item[Modeling:] Here is where the model is designed and selected. It includes choosing algorithms, planning the approach and shaping how data will be processed for predictions.
  \item[Training:] The model is trained using labeled data. The quality of training data impacts the model’s performance
  \item[Evaluation:] The now trained model is tested on unseen data. Standard or custom metrics are used to evaluate model’s performance and compare models.
  \item[Prediction:] After evaluation, the model is applied to generate predictions for unknown cases. Its effectiveness is measured using metrics like accuracy, F1 score or loss.
  \item[Interpretation:] Predictions are translated into understandable insights through post-processing. This steps turns raw results into knowledge for decision-making
\end{description}

In the context of this thesis , we will be working with a data classification pipeline, which is a specialized form of an ML pipeline. It combines the ground steps of ETL pipeline with the enhanced predictions of an ML pipeline to create a pipeline that focuses specifically on assigning labels to input data.


\section{ML Models}
\label{sec:background:ml_models}
A machine learning (ML) model is a program designed to identify patterns or make decisions based on data \cite{databricks:2022}. This is achieved through a process called training, in which the algorithm learns from large datasets by optimizing its parameters to capture relevant outputs.The result of this process is the ML model itself , which encodes the learned rules in order to perform the intended task. ML models can be classified in four categories \cite{khoei:2023}:

\begin{table}[htbp]
  \centering
  \begin{tabularx}{\linewidth}{@{}lX@{}}
    \toprule
    \textbf{Category}        & \textbf{Description}                                                                                                                                                                                                                                                                                                                                                      \\
    \midrule
    Supervised-learning      & Models are trained on labeled input-output pairs , allowing them to make predictions on unseen data. The process relies on human created labels and guidance, which play a crucial role in building accurate models based on known parameters.                                                                                                                            \\
    \addlinespace
    Semi-supervised learning & Combines a small set of labeled data together with a larger pool of unlabeled data to improve predictions or classifications. This approach is particularly useful when labeling is costly or time consuming . While semi-supervised learning can often outperform purely supervised models, its effectiveness depends on the quality of initial labels \cite{zhou:2021}. \\
    \addlinespace
    Unsupervised learning    & Analyses unlabeled data to uncover hidden groupings within a dataset. Clustering techniques are commonly applied in this context to identify meaningful relationships and extract knowledge from data without predefined labels.\cite{wu:2022}                                                                                                                            \\
    \addlinespace
    Reinforcement learning   & The model learns to take optimal actions in an interactive environment based on feedback from its own experiences. Instead of relying on labeled data, the model explores action sequences independently to maximize cumulative rewards. \cite{talaeikhoei:2023}                                                                                                          \\
    \bottomrule
  \end{tabularx}
  \caption{ML model categories.}
  \label{tab:ml_model_categories}
\end{table}


\section{NLP \& Text Classification}
\label{sec:background:nlp_text_classification}
In machine learning , classification refers to a predictive model task, where a class label is assigned to a given example \cite{sarker:2021} . Classification can be applied to both structured and unstructured data. There are 3 kinds of classification tasks:

\begin{enumerate}
  \item \textbf{Binary classification.} Involves exactly 2 classes, such as true/false or yes/no. Typically one label represents the normal state while the other an abnormal state \cite{han:2011}. A common example is spam filtering where it distinguishes between “spam” or “not spam”
  \item \textbf{ Multiclass classification:} This task involves more than two possible class labels. Unlike binary tasks, which involves normal versus abnormal outcomes, here we assign a class within a broader set of categories .
  \item \textbf{Multi-label classification :} Here each example may be associated with multiple labels simultaneously. For example , a google news article that could simultaneously  belong to category “city name”, “technology” and “latest news”
\end{enumerate}

Natural Language Processing (NLP) refers to the ability of computers to read, interpret and process human language in written or spoken  \cite{otter:2021} . It enables tasks such as text reading, speech recognition or document summarization. Sentiment analysis, a.k.a opinion mining, is a subfield on NLP that focuses on identifying attitudes , opinions or emotions from text sources. As a machine learning task , sentiment analysis typically classifies text into polarity categories (e.g positive, negative or neutral) but can also detect emotions such as happiness , sadness or anger \cite{ravi:2015}
